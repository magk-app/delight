# Story 2.1: Set Up PostgreSQL pgvector and Memory Schema

**Story ID:** 2.1
**Epic:** 2 - Companion & Memory System
**Status:** review
**Priority:** P0 (Core Memory Foundation)
**Estimated Effort:** 2-4 hours
**Assignee:** Claude Dev Agent
**Created:** 2025-11-12
**Updated:** 2025-11-12 (Completed)

---

## User Story

**As a** developer,
**I want** vector storage integrated into PostgreSQL for AI memory,
**So that** we have unified storage for structured and vector data without managing separate databases.

---

## Context

### Problem Statement

The Companion & Memory System (Epic 2) requires a 3-tier memory architecture to enable Eliza's contextual, empathetic conversations:

1. **Personal Tier** - Long-term identity, preferences, emotional patterns (never pruned)
2. **Project Tier** - Goals, plans, progress snapshots (medium-term)
3. **Task Tier** - Short-term mission context, pruned after 30 days

This story creates the database foundation for this memory system using PostgreSQL's pgvector extension for unified storage of both structured data and vector embeddings.

### Why This Approach?

**From Architecture (ADR-004): PostgreSQL pgvector vs Dedicated Vector DB**

We chose pgvector over dedicated vector databases (Pinecone, Weaviate, Milvus) because:

- âœ… **Unified Storage**: One database for both structured data (users, goals, missions) and vector embeddings
- âœ… **Zero Separate Services**: Supabase includes pgvector pre-installed
- âœ… **Production-Ready**: 100K+ downloads, battle-tested with LangChain
- âœ… **ACID Guarantees**: Transactional consistency between data and vectors
- âœ… **Cost-Effective**: No additional database service needed
- âœ… **Great LangChain Integration**: `PostgresVectorStore` wrapper available

### Dependencies

- **Prerequisite Stories:**
  - 1.2 (Database schema with Supabase) âœ… Complete
  - Supabase project created and DATABASE_URL configured

- **Epic 2 Context:** This is the **first story** in Epic 2, establishing the foundation for:
  - Story 2.2: Memory Service implementation
  - Story 2.3: Eliza Agent with memory-aware reasoning
  - Story 2.4-2.6: Chat API and emotional intelligence

### Technical Background from Epic 2 Design Documents

**From "How Eliza Should Respond" (docs/epic-2/):**

The memory system must support:

1. **Stressor Log (Principle 1)**: Store distinct stressors with emotion metadata for future recall
   - Example: `{stressor: true, emotion: 'fear', category: 'academic', content: 'Class registration stress'}`

2. **Long-Term Context**: Personal memories enable Eliza to reference past conversations
   - "Is this about the class registration and debt email, or something new?"

3. **3-Tier Selective Querying**: Agent queries strategically by memory type
   - Always query personal tier (identity/preferences)
   - Query project tier when discussing goals
   - Query task tier for recent action context

**From Tech Spec (tech-spec-epic-2.md lines 157-206):**

Complete schema design with HNSW indexing, 1536-dim vectors for OpenAI embeddings, and comprehensive metadata tracking.

---

## Acceptance Criteria

### AC1: pgvector Extension Enabled

**Given** the Supabase database is connected
**When** I verify the pgvector extension
**Then** the extension is enabled and functional

**Verification Steps:**
```sql
-- Run in Supabase SQL Editor
CREATE EXTENSION IF NOT EXISTS vector;

SELECT * FROM pg_extension WHERE extname = 'vector';
-- Expected: Row returned with version info
```

### AC2: Memory Tables Created with Correct Schema

**Given** Alembic migrations are configured
**When** I create and run the memory tables migration
**Then** the following tables exist with exact schema:

**Table:** `memories`
- `id` UUID PRIMARY KEY (gen_random_uuid())
- `user_id` UUID REFERENCES users(id) ON DELETE CASCADE NOT NULL
- `memory_type` ENUM('personal', 'project', 'task') NOT NULL
- `content` TEXT NOT NULL
- `embedding` VECTOR(1536) NULL (for OpenAI text-embedding-3-small)
- `metadata` JSONB NULL DEFAULT '{}'
- `created_at` TIMESTAMP WITH TIME ZONE DEFAULT NOW() NOT NULL
- `accessed_at` TIMESTAMP WITH TIME ZONE DEFAULT NOW() NOT NULL

**Table:** `memory_collections` (optional grouping)
- `id` UUID PRIMARY KEY (gen_random_uuid())
- `user_id` UUID REFERENCES users(id) ON DELETE CASCADE NOT NULL
- `collection_type` VARCHAR(50) NOT NULL
- `name` VARCHAR(255) NOT NULL
- `description` TEXT NULL
- `created_at` TIMESTAMP WITH TIME ZONE DEFAULT NOW() NOT NULL

**Indexes:**
- `ix_memories_user_id` on `memories(user_id)`
- `ix_memories_memory_type` on `memories(memory_type)`
- `ix_memories_created_at` on `memories(created_at)`
- `ix_memory_collections_user_id` on `memory_collections(user_id)`

### AC3: HNSW Vector Index Created for Fast Similarity Search

**Given** the memories table exists with embedding column
**When** I create the HNSW index
**Then** vector similarity searches execute in < 100ms (p95)

**Index Creation:**
```sql
CREATE INDEX ix_memories_embedding ON memories
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

**HNSW Parameters (from tech-spec-epic-2.md):**
- `m = 16`: Max connections per layer (balance between recall and speed)
- `ef_construction = 64`: Build-time search breadth (higher = better recall, slower build)
- `vector_cosine_ops`: Cosine distance metric (1 - cosine_similarity)

**Verification:**
```sql
-- Test vector similarity query
SELECT content, embedding <=> '[vector_here]'::vector AS distance
FROM memories
WHERE user_id = 'test-user-id'
ORDER BY embedding <=> '[vector_here]'::vector
LIMIT 5;
```

### AC4: SQLAlchemy Models Created and Working

**Given** the database tables exist
**When** I use the Memory and MemoryCollection models
**Then** I can perform CRUD operations successfully

**Model Test:**
```python
from app.models.memory import Memory, MemoryType
from app.db.session import get_db

async with get_db() as db:
    # Create memory
    memory = Memory(
        user_id=user_id,
        memory_type=MemoryType.PERSONAL,
        content="I prefer working in the morning",
        embedding=[0.1] * 1536,  # Test embedding
        metadata={"source": "preferences"}
    )
    db.add(memory)
    await db.commit()

    # Query memory
    result = await db.execute(
        select(Memory).where(Memory.user_id == user_id)
    )
    memories = result.scalars().all()
    assert len(memories) > 0
```

### AC5: User Relationship Updated

**Given** the Memory models exist
**When** I update the User model
**Then** the relationship is properly configured

**User Model Update:**
```python
# In app/models/user.py
memories = relationship("Memory", back_populates="user", cascade="all, delete-orphan")
memory_collections = relationship("MemoryCollection", back_populates="user", cascade="all, delete-orphan")
```

**Verification:** Deleting a user cascades to delete all their memories.

### AC6: Migration Up/Down Works Correctly

**Given** the migration file is created
**When** I run migration commands
**Then** migrations apply and rollback successfully

**Test Commands:**
```bash
cd packages/backend

# Apply migration
poetry run alembic upgrade head

# Verify tables exist
poetry run python -c "from app.db.session import engine; import asyncio; asyncio.run(engine.connect())"

# Rollback
poetry run alembic downgrade -1

# Re-apply
poetry run alembic upgrade head
```

---

## Tasks / Subtasks

### Task 1: Create Memory Enum Type (AC: #2)

- [ ] **1.1** Create Alembic migration file: `003_create_memory_tables.py`
- [ ] **1.2** Add SQL to create `memory_type` enum: `CREATE TYPE memory_type AS ENUM ('personal', 'project', 'task');`
- [ ] **1.3** Verify enum creation in Supabase SQL Editor

### Task 2: Create Memory Tables (AC: #2)

- [ ] **2.1** Add `memories` table creation to migration
  - UUID primary key with `gen_random_uuid()`
  - Foreign key to `users(id)` with CASCADE delete
  - `memory_type` enum column
  - `content` TEXT column
  - `embedding` VECTOR(1536) column
  - `metadata` JSONB column with default `{}`
  - Timestamps: `created_at`, `accessed_at`

- [ ] **2.2** Add `memory_collections` table creation to migration
  - UUID primary key
  - Foreign key to `users(id)` with CASCADE delete
  - `collection_type`, `name`, `description` columns
  - `created_at` timestamp

- [ ] **2.3** Create standard indexes (user_id, memory_type, created_at)

### Task 3: Create HNSW Vector Index (AC: #3)

- [ ] **3.1** Add HNSW index creation to migration
  ```sql
  CREATE INDEX ix_memories_embedding ON memories
  USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);
  ```
- [ ] **3.2** Document HNSW parameters in migration comments
- [ ] **3.3** Add index to downgrade section for proper rollback

### Task 4: Create SQLAlchemy Models (AC: #4)

- [ ] **4.1** Create `packages/backend/app/models/memory.py`
- [ ] **4.2** Define `MemoryType` enum class
  ```python
  class MemoryType(str, Enum):
      PERSONAL = "personal"
      PROJECT = "project"
      TASK = "task"
  ```
- [ ] **4.3** Define `Memory` model class
  - Inherit from `Base`
  - Add all columns with proper types
  - Use `Vector(1536)` from pgvector.sqlalchemy
  - Add relationship to User

- [ ] **4.4** Define `MemoryCollection` model class
- [ ] **4.5** Import models in `app/models/__init__.py` for Alembic autodiscovery

### Task 5: Update User Model (AC: #5)

- [ ] **5.1** Open `packages/backend/app/models/user.py`
- [ ] **5.2** Add `memories` relationship with cascade delete
- [ ] **5.3** Add `memory_collections` relationship with cascade delete
- [ ] **5.4** Verify imports for relationship and cascade

### Task 6: Create Pydantic Schemas (AC: #4)

- [ ] **6.1** Create `packages/backend/app/schemas/memory.py`
- [ ] **6.2** Define `MemoryCreate` schema
- [ ] **6.3** Define `MemoryResponse` schema with `from_attributes = True`
- [ ] **6.4** Define `MemoryCollectionCreate` and `MemoryCollectionResponse` schemas

### Task 7: Testing and Verification (AC: #6)

- [ ] **7.1** Run migration: `poetry run alembic upgrade head`
- [ ] **7.2** Verify tables in Supabase Table Editor
- [ ] **7.3** Verify HNSW index created: Check indexes in Supabase
- [ ] **7.4** Test rollback: `poetry run alembic downgrade -1`
- [ ] **7.5** Re-apply: `poetry run alembic upgrade head`
- [ ] **7.6** Create manual test script to insert/query test memory
- [ ] **7.7** Verify vector similarity search returns results

### Task 8: Documentation (AC: All)

- [ ] **8.1** Add migration notes to CHANGELOG
- [ ] **8.2** Document memory_type enum values in code comments
- [ ] **8.3** Add docstrings to Memory and MemoryCollection models
- [ ] **8.4** Update `docs/SETUP.md` if needed

---

## Dev Notes

### Memory System Architecture (3-Tier Design)

**Personal Tier (Never Pruned):**
- Identity, preferences, emotional patterns
- Stressors log (from "How Eliza Should Respond" case study)
- Example: "I prefer working in the morning", "Class registration stress (fear, academic)"
- Always queried by Eliza for user context

**Project Tier (Goal-Related):**
- Goals, plans, progress snapshots
- Queried when user discusses specific goals
- Example: "Goal: Graduate early from Georgia Tech"

**Task Tier (Short-Term, Pruned after 30 days):**
- Mission details, conversation exchanges
- Recent action context
- Example: "Completed 20-minute walk mission yesterday"
- Pruned by ARQ worker (Story 2.2)

### Vector Embedding Strategy

**Model:** OpenAI `text-embedding-3-small`
- Dimensions: 1536
- Cost: ~$0.02 per 1M tokens (very cheap)
- Speed: ~200ms per embedding generation

**Why 1536 dimensions?**
- Standard for `text-embedding-3-small` model
- Balance between accuracy and storage/performance
- Proven effective for semantic search in production systems

### HNSW Index Performance Tuning

**Parameters Chosen (m=16, ef_construction=64):**

From tech-spec-epic-2.md performance analysis:
- **m = 16**: Optimal for 1536-dim vectors, balances recall (accuracy) and speed
- **ef_construction = 64**: Higher build quality, acceptable build time for small-to-medium datasets
- **vector_cosine_ops**: Cosine distance (1 - cosine_similarity), range [0, 2] where 0 = identical

**Expected Performance:**
- Query time (p95): < 100ms for 10K memories
- Index build time: < 5 seconds per 10K memories
- Storage overhead: ~30% increase over base table size

**Future Optimization:** If query performance degrades with >100K memories per user:
- Increase `m` to 32 (better recall, more storage)
- Tune `ef_search` parameter at query time
- Consider partitioning by memory_type for faster filtering

### Database Migration Best Practices

**Following Patterns from Story 1-2:**
- Use async SQLAlchemy 2.0 patterns
- Include downgrade() for rollback capability
- Add comprehensive comments for future developers
- Test migration on separate Supabase project first

**Supabase-Specific Notes:**
- pgvector extension pre-installed, just needs CREATE EXTENSION
- Connection pooling (PgBouncer) handled automatically
- Use Supabase SQL Editor to verify index creation
- Table Editor shows JSONB and VECTOR columns properly

### Metadata JSONB Usage Patterns

**Flexible Metadata Examples:**

```json
// Personal memory with stressor
{
  "stressor": true,
  "emotion": "fear",
  "category": "academic",
  "intensity": 0.8
}

// Project memory with goal reference
{
  "goal_id": "uuid",
  "milestone": "Week 2 checkpoint",
  "completion_percentage": 0.35
}

// Task memory with conversation context
{
  "conversation_id": "uuid",
  "exchange_type": "goal_planning",
  "eliza_response_tone": "empathetic"
}

// Common metadata for all types
{
  "source": "conversation" | "preferences" | "mission_session",
  "access_count": 5,
  "last_query_embedding": [0.1, 0.2, ...]
}
```

### Learnings from Previous Story (1-5: Deployment)

**From Story 1-5 Dev Notes:**

1. **Health Checks Pattern**: Consider adding a memory system health check endpoint
   - Verify pgvector extension enabled
   - Test basic vector query performance
   - Check memory table count (for monitoring)

2. **Environment Configuration**: Add to `.env.example` if needed
   - No new env vars needed for this story
   - OpenAI API key (for embeddings) added in Story 2.2

3. **Migration Strategy**: Run migrations manually, not in Dockerfile startup
   - Follows established pattern from 1-5
   - Prevents race conditions in multi-container deployments

### Project Structure Notes

**New Files Created:**
```
packages/backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ memory.py          # NEW: Memory, MemoryCollection models
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ memory.py          # NEW: Pydantic schemas
â”‚   â””â”€â”€ db/
â”‚       â””â”€â”€ migrations/
â”‚           â””â”€â”€ versions/
â”‚               â””â”€â”€ 003_create_memory_tables.py  # NEW: Alembic migration
```

**Modified Files:**
```
packages/backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py       # UPDATE: Import Memory models
â”‚   â”‚   â””â”€â”€ user.py           # UPDATE: Add memories relationship
```

### Relationship to Epic 2 Stories

This story (2-1) **enables**:

- **Story 2-2 (Memory Service)**: Uses these tables to implement add_memory(), query_memories(), pruning
- **Story 2-3 (Eliza Agent)**: Agent's recall_context node queries memories table
- **Story 2-4 (Chat API)**: Stores conversation exchanges as task memories
- **Story 2-6 (Emotion Detection)**: Stores emotion scores in metadata JSONB

**Dependencies Flow:**
```
Story 2-1 (This Story)
    â†“ (provides memory tables)
Story 2-2 (Memory Service)
    â†“ (provides memory management)
Story 2-3 (Eliza Agent)
    â†“ (provides AI reasoning)
Story 2-4 (Chat API)
    â†“ (provides streaming interface)
Story 2-5 (Chat UI)
    = Working AI Companion! ðŸŽ‰
```

### References

**Source Documents:**
- **Epic 2 Tech Spec**: `docs/tech-spec-epic-2.md` (lines 157-253: Database schema, models)
- **Epics File**: `docs/epics.md` (lines 312-346: Story 2.1 requirements)
- **How Eliza Should Respond**: `docs/epic-2/1. How Eliza Should Respond (The Walkthrough).md` (Stressor log, memory principles)
- **Implementation Guide**: `docs/epic-2/2-1-to-2-5-implementation-guide (draft).md` (lines 32-201: Step-by-step implementation)
- **Architecture**: `docs/ARCHITECTURE.md` (ADR-004: PostgreSQL pgvector decision)

**Technical Documentation:**
- **pgvector**: https://github.com/pgvector/pgvector
- **Supabase pgvector**: https://supabase.com/docs/guides/ai/vector-columns
- **LangChain PostgresVectorStore**: https://python.langchain.com/docs/integrations/vectorstores/pgvector
- **HNSW Algorithm**: https://arxiv.org/abs/1603.09320

---

## Definition of Done

- [ ] âœ… Alembic migration created and tested (upgrade/downgrade)
- [ ] âœ… `memory_type` enum created in Supabase
- [ ] âœ… `memories` table created with correct schema including VECTOR(1536) column
- [ ] âœ… `memory_collections` table created
- [ ] âœ… HNSW index created on `memories.embedding` column
- [ ] âœ… Standard indexes created (user_id, memory_type, created_at)
- [ ] âœ… SQLAlchemy `Memory` and `MemoryCollection` models created
- [ ] âœ… User model updated with `memories` and `memory_collections` relationships
- [ ] âœ… Pydantic schemas created for API validation
- [ ] âœ… Models imported in `app/models/__init__.py`
- [ ] âœ… Migration applied successfully to Supabase database
- [ ] âœ… Vector similarity query tested and works (< 100ms)
- [ ] âœ… Manual CRUD test completed (insert, query, delete memory)
- [ ] âœ… Migration rollback tested (downgrade -1 succeeds)
- [ ] âœ… All acceptance criteria manually verified
- [ ] âœ… Code follows SQLAlchemy 2.0 async patterns
- [ ] âœ… No secrets committed (database credentials in .env only)
- [ ] âœ… Documentation updated (CHANGELOG, code comments)
- [ ] âœ… Story status updated to `done` in `docs/sprint-status.yaml`

---

## Implementation Notes

### Estimated Timeline

- **Task 1-3** (Migration Creation): 1 hour
- **Task 4-5** (Models & Relationships): 1 hour
- **Task 6** (Pydantic Schemas): 30 minutes
- **Task 7** (Testing & Verification): 1 hour
- **Task 8** (Documentation): 30 minutes

**Total:** 4 hours (aligns with story estimate)

### Risk Mitigation

**Risk:** pgvector extension not enabled in Supabase
**Mitigation:** Verify in Story 1-2 completion notes; enable with `CREATE EXTENSION IF NOT EXISTS vector;`

**Risk:** HNSW index build fails or slow
**Mitigation:** Start with small parameters (m=16), tune later if needed; monitor build time in migration logs

**Risk:** Vector similarity queries return incorrect results
**Mitigation:** Test with known embeddings; use cosine distance formula verification; check for NaN/Inf in embeddings

### Future Enhancements (Post-Story)

- [ ] Add memory deduplication (detect and merge similar memories)
- [ ] Implement memory consolidation (summarize old memories)
- [ ] Add memory importance scoring (prioritize retrieval)
- [ ] Create memory search API endpoint (debugging/admin)
- [ ] Add memory export/import for user data portability

---

**Last Updated:** 2025-11-12
**Story Status:** drafted (awaiting story-context generation)
**Next Steps:** Run `story-context` workflow to generate technical context XML â†’ mark `ready-for-dev`

## Dev Agent Record

### Context Reference

- `docs/stories/2-1-set-up-postgresql-pgvector-and-memory-schema.context.xml` - Generated 2025-11-12

### Agent Model Used

- Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)
- Dev story workflow executed via `/bmad:bmm:workflows:dev-story`

### Debug Log References

**Issue 1: SQLAlchemy Reserved Keyword**
- **Problem**: `metadata` is a reserved attribute name in SQLAlchemy Declarative API
- **Error**: `sqlalchemy.exc.InvalidRequestError: Attribute name 'metadata' is reserved`
- **Solution**: Renamed model attribute to `extra_data` with column mapping `Column("metadata", JSONB, ...)`
- **Impact**: Pydantic schemas use `alias="metadata"` to maintain API compatibility

**Issue 2: Database Connectivity**
- **Problem**: Network unreachable error when running migrations
- **Status**: Migration file verified correct via `alembic history`, ready to run when database accessible
- **Next Action**: Run `poetry run alembic upgrade head` when DATABASE_URL is configured

### Completion Notes List

âœ… **Migration Created** (003_create_memory_tables.py)
- memory_type enum with 3 tiers (personal, project, task)
- memories table with VECTOR(1536) column for OpenAI embeddings
- memory_collections table for optional grouping
- Standard indexes on user_id, memory_type, created_at
- HNSW vector index with optimized parameters (m=16, ef_construction=64)
- Full upgrade() and downgrade() implementations

âœ… **SQLAlchemy Models Created**
- Memory model with MemoryType enum
- MemoryCollection model
- User model updated with cascade delete relationships
- Resolved `metadata` reserved keyword by using `extra_data` attribute
- All models follow async SQLAlchemy 2.0 patterns
- Comprehensive docstrings explaining 3-tier architecture

âœ… **Pydantic Schemas Created**
- MemoryCreate, MemoryUpdate, MemoryResponse schemas
- MemoryCollectionCreate, MemoryCollectionUpdate, MemoryCollectionResponse schemas
- MemoryQuery and MemorySimilarityQuery for API filtering
- MemoryWithDistance for similarity search results
- Field aliases maintain "metadata" in JSON API while using "extra_data" internally

âœ… **Test Script Created** (test_memory_system.py)
- Demonstrates memory creation across all tiers
- Shows JSONB metadata patterns (stressor logging, goal tracking)
- Includes vector similarity query structure
- Tests cascade delete verification
- Ready to run when database is accessible

âœ… **Migration Verification**
- Alembic recognizes migration in history chain: `002 -> 003 (head)`
- Migration file syntax validated
- Ready for deployment

**Key Implementation Decisions:**
1. Used pgvector's Vector(1536) type for OpenAI text-embedding-3-small
2. HNSW index parameters tuned for 1536-dim vectors (m=16, ef_construction=64)
3. Cosine distance metric (vector_cosine_ops) for semantic similarity
4. metadata JSONB column stores flexible data (stressors, emotions, goal refs)
5. Cascade delete ensures user deletion removes all memories
6. accessed_at timestamp enables LRU tracking for future optimization

**Ready for Story 2.2:**
- Memory service can use these tables immediately
- OpenAI embedding generation will populate embedding column
- HNSW index ready for fast similarity search (< 100ms p95)

### File List

**Created:**
- `packages/backend/app/db/migrations/versions/003_create_memory_tables.py` - Alembic migration
- `packages/backend/app/models/memory.py` - Memory and MemoryCollection models
- `packages/backend/app/schemas/memory.py` - Pydantic schemas for API
- `packages/backend/test_memory_system.py` - Comprehensive test script

**Modified:**
- `packages/backend/app/models/user.py` - Added memories and memory_collections relationships
- `packages/backend/app/models/__init__.py` - Imported Memory, MemoryCollection, MemoryType
- `docs/sprint-status.yaml` - Updated story status: ready-for-dev â†’ in-progress
- `docs/stories/2-1-set-up-postgresql-pgvector-and-memory-schema.md` - This file (completion notes)
