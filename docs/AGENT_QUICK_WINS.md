# Agent System Quick Wins
## High-Impact Improvements for Next Sprint

**Timeline:** 1-2 sprints (2-4 weeks)
**Effort:** Low to Medium
**Impact:** High

---

## Quick Win #1: Parallel Tool Execution (1 day)

**Current Problem:**
```python
# Sequential execution - slow!
for tool_name in selected_tools:
    result = await self.tool_store.execute_tool(tool_name, ...)
    # Waits for each tool to complete
```

**Quick Fix:**
```python
# Parallel execution - 3x faster!
import asyncio

async def _tool_execution_node(self, state: GraphState) -> GraphState:
    selected_tools = state.get("selected_tools", [])

    # Create tasks for all tools
    tasks = []
    for tool_name in selected_tools:
        input_data = self._generate_tool_input(state, tool_name)
        if input_data:
            task = self.tool_store.execute_tool(
                tool_name,
                input_data,
                state.get("available_information", {})
            )
            tasks.append((tool_name, task))

    # Execute all tools in parallel
    results = await asyncio.gather(*[t[1] for t in tasks], return_exceptions=True)

    # Process results
    for (tool_name, _), result in zip(tasks, results):
        if isinstance(result, Exception):
            logger.error(f"Tool {tool_name} failed", exc_info=result)
            state["error_message"] = f"Tool execution failed: {str(result)}"
        else:
            execution = ToolExecution(
                tool_name=tool_name,
                input_data=input_data,
                output=result.result if result.success else None,
                success=result.success,
                error=result.error
            )
            state["tool_executions"].append(execution)

    return state
```

**Impact:**
- 3x faster for multi-tool requests
- Better user experience
- Scales to more tools

**Effort:** 4 hours

---

## Quick Win #2: Tool Result Caching (4 hours)

**Current Problem:**
- Same tool + same input = re-execute every time
- Wastes compute + money

**Quick Fix:**
```python
from functools import lru_cache
import hashlib
import json

class CachedToolStore(ToolStore):
    """Tool store with result caching"""

    def __init__(self):
        super().__init__()
        self.cache = {}  # Or use Redis for distributed cache
        self.cache_ttl = 300  # 5 minutes

    def _cache_key(self, tool_name: str, input_data: Dict[str, Any]) -> str:
        """Generate cache key from tool + input"""
        key_data = {
            "tool": tool_name,
            "input": input_data
        }
        return hashlib.md5(json.dumps(key_data, sort_keys=True).encode()).hexdigest()

    async def execute_tool(
        self,
        tool_name: str,
        input_data: ToolInput,
        context: Dict[str, Any]
    ) -> ToolOutput:
        """Execute tool with caching"""

        # Check cache first
        cache_key = self._cache_key(tool_name, input_data.model_dump())

        if cache_key in self.cache:
            cached_result, cached_at = self.cache[cache_key]

            # Check if cache is still valid
            if (datetime.utcnow() - cached_at).seconds < self.cache_ttl:
                logger.info(f"Cache hit for {tool_name}")
                return cached_result

        # Cache miss - execute tool
        result = await super().execute_tool(tool_name, input_data, context)

        # Cache successful results only
        if result.success:
            self.cache[cache_key] = (result, datetime.utcnow())

        return result
```

**Impact:**
- Faster responses for repeated queries
- Lower costs (avoid redundant LLM calls)
- Better UX (instant results for cached queries)

**Effort:** 4 hours

---

## Quick Win #3: Structured Logging (2 hours)

**Current Problem:**
```python
print(f"[INITIALIZE] Goal: {goal.description}")  # Not searchable, not filterable
```

**Quick Fix:**
```python
import structlog
from app.core.logging_config import get_logger

logger = get_logger(__name__)

async def _initialize_node(self, state: GraphState) -> GraphState:
    logger.info(
        "agent_state_transition",
        state="INITIALIZE",
        goal_description=state["goal"].description if state.get("goal") else None,
        user_id=state.get("user_id"),
        request_id=state.get("request_id"),
        iteration=state.get("iterations", 0)
    )

    # ... rest of node implementation
```

**Create logging config:**
```python
# app/core/logging_config.py
import structlog

def setup_logging():
    """Configure structured logging"""

    structlog.configure(
        processors=[
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.JSONRenderer()
        ],
        wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(),
        cache_logger_on_first_use=True,
    )

def get_logger(name: str):
    return structlog.get_logger(name)
```

**Impact:**
- Easy debugging (search logs by user_id, request_id)
- Production monitoring (aggregate by state, error type)
- Performance analysis (track state transition times)

**Effort:** 2 hours

---

## Quick Win #4: Request Timeout & Circuit Breaker (3 hours)

**Current Problem:**
- No timeout for LLM calls
- Single LLM failure can hang agent forever

**Quick Fix:**
```python
from asyncio import wait_for, TimeoutError
import asyncio

class TimeoutConfig:
    LLM_CALL_TIMEOUT = 30  # 30 seconds
    TOOL_EXECUTION_TIMEOUT = 60  # 1 minute
    AGENT_EXECUTION_TIMEOUT = 120  # 2 minutes

async def _synthesis_node(self, state: GraphState) -> GraphState:
    try:
        # Wrap LLM call with timeout
        response = await wait_for(
            llm.ainvoke(synthesis_prompt),
            timeout=TimeoutConfig.LLM_CALL_TIMEOUT
        )
        state["final_response"] = response

    except TimeoutError:
        logger.error(
            "llm_timeout",
            node="synthesis",
            user_id=state.get("user_id"),
            timeout=TimeoutConfig.LLM_CALL_TIMEOUT
        )

        # Fallback response
        state["final_response"] = self._generate_fallback_response(state)
        state["error_message"] = "LLM timeout - using fallback"

    return state

def _generate_fallback_response(self, state: GraphState) -> str:
    """Generate simple fallback when LLM fails"""
    goal = state.get("goal")
    tool_outputs = state.get("tool_outputs", {})

    if tool_outputs:
        # Simple template response
        return f"I've processed your request for: {goal.description}\n\n" + \
               "\n".join([f"- {tool}: {output}" for tool, output in tool_outputs.items()])
    else:
        return f"I'm working on: {goal.description}\nBut encountered some issues. Please try again."
```

**Impact:**
- No hanging requests
- Better error handling
- Improved reliability

**Effort:** 3 hours

---

## Quick Win #5: Cost Tracking (4 hours)

**Current Problem:**
- No visibility into LLM costs
- Can't measure against $0.03/day target

**Quick Fix:**
```python
from langchain.callbacks import get_openai_callback
from app.models.usage_tracking import UsageLog

async def run(self, user_request: str, user_id: UUID) -> Dict[str, Any]:
    """Run agent with cost tracking"""

    # Track token usage
    with get_openai_callback() as cb:
        final_state = await self.graph.ainvoke(initial_state)

    # Log usage to database
    usage_log = UsageLog(
        user_id=user_id,
        operation_type="agent_execution",
        tokens_used=cb.total_tokens,
        cost_usd=cb.total_cost,
        metadata={
            "goal": final_state["goal"].description,
            "tools_used": final_state["selected_tools"],
            "confidence": final_state["confidence"]
        }
    )
    await db.add(usage_log)
    await db.commit()

    logger.info(
        "agent_execution_cost",
        user_id=user_id,
        tokens=cb.total_tokens,
        cost=cb.total_cost,
        prompt_tokens=cb.prompt_tokens,
        completion_tokens=cb.completion_tokens
    )

    return {
        "success": final_state.get("current_state") == AgentState.COMPLETION,
        "response": final_state.get("final_response"),
        "cost": cb.total_cost,
        "tokens": cb.total_tokens
    }
```

**Database model:**
```python
# app/models/usage_tracking.py
from sqlalchemy import Column, UUID, String, Integer, Float, TIMESTAMP, JSON
from app.db.base import Base

class UsageLog(Base):
    __tablename__ = "usage_logs"

    id = Column(UUID, primary_key=True, server_default=text("gen_random_uuid()"))
    user_id = Column(UUID, ForeignKey("users.id"), nullable=False)
    operation_type = Column(String, nullable=False)  # "agent_execution", "nudge_generation", etc.
    tokens_used = Column(Integer, nullable=False)
    cost_usd = Column(Float, nullable=False)
    metadata = Column(JSON)
    created_at = Column(TIMESTAMP(timezone=True), server_default=text("now()"))
```

**Create dashboard endpoint:**
```python
@router.get("/admin/cost-dashboard")
async def get_cost_dashboard(days: int = 7):
    """Get cost metrics for admin dashboard"""

    # Query usage logs
    daily_costs = await db.execute(
        select(
            func.date(UsageLog.created_at).label("date"),
            func.sum(UsageLog.cost_usd).label("total_cost"),
            func.sum(UsageLog.tokens_used).label("total_tokens"),
            func.count(UsageLog.id).label("operation_count")
        )
        .where(UsageLog.created_at >= datetime.utcnow() - timedelta(days=days))
        .group_by(func.date(UsageLog.created_at))
    )

    # Calculate per-user average
    per_user_cost = await db.execute(
        select(
            func.avg(subq.c.daily_cost)
        )
        .select_from(
            select(
                UsageLog.user_id,
                func.date(UsageLog.created_at).label("date"),
                func.sum(UsageLog.cost_usd).label("daily_cost")
            )
            .group_by(UsageLog.user_id, func.date(UsageLog.created_at))
            .subquery()
        )
    )

    return {
        "daily_breakdown": daily_costs.all(),
        "avg_cost_per_user_per_day": per_user_cost.scalar(),
        "target": 0.03,
        "status": "on_track" if per_user_cost.scalar() < 0.03 else "over_budget"
    }
```

**Impact:**
- Visibility into costs
- Track against ADR-009 target
- Identify expensive operations
- Optimize cost over time

**Effort:** 4 hours

---

## Quick Win #6: Better Error Messages (2 hours)

**Current Problem:**
```python
# Generic error, not helpful
return {"success": False, "error": "Agent execution failed"}
```

**Quick Fix:**
```python
class AgentError(Exception):
    """Base exception for agent errors"""

    def __init__(self, message: str, error_code: str, details: Dict[str, Any]):
        super().__init__(message)
        self.error_code = error_code
        self.details = details

class GoalParsingError(AgentError):
    """Raised when goal cannot be parsed"""
    def __init__(self, user_request: str):
        super().__init__(
            message="I couldn't understand your goal. Could you rephrase it?",
            error_code="GOAL_PARSING_ERROR",
            details={"user_request": user_request}
        )

class ToolExecutionError(AgentError):
    """Raised when tool execution fails"""
    def __init__(self, tool_name: str, error: str):
        super().__init__(
            message=f"I had trouble using the {tool_name} tool. Let's try a different approach.",
            error_code="TOOL_EXECUTION_ERROR",
            details={"tool_name": tool_name, "error": error}
        )

# Usage in nodes:
async def _tool_execution_node(self, state: GraphState) -> GraphState:
    try:
        result = await self.tool_store.execute_tool(...)

        if not result.success:
            raise ToolExecutionError(tool_name, result.error)

    except ToolExecutionError as e:
        state["error_message"] = e.message
        state["error_code"] = e.error_code
        state["error_details"] = e.details
        state["current_state"] = AgentState.ERROR

        logger.error(
            "tool_execution_error",
            error_code=e.error_code,
            details=e.details,
            user_id=state.get("user_id")
        )

    return state
```

**Impact:**
- Better user experience (helpful error messages)
- Easier debugging (error codes)
- Actionable feedback (user knows what to do)

**Effort:** 2 hours

---

## Quick Win #7: State Persistence (6 hours)

**Current Problem:**
- Agent execution is ephemeral
- Can't resume if agent crashes
- No conversation history

**Quick Fix:**
```python
# app/models/agent_execution.py
from sqlalchemy import Column, UUID, String, Integer, Float, TIMESTAMP, JSON
from app.db.base import Base

class AgentExecution(Base):
    """Persistent storage for agent executions"""
    __tablename__ = "agent_executions"

    id = Column(UUID, primary_key=True, server_default=text("gen_random_uuid()"))
    user_id = Column(UUID, ForeignKey("users.id"), nullable=False)
    user_request = Column(String, nullable=False)

    # Agent state
    current_state = Column(String)  # "INITIALIZE", "PLANNING", etc.
    goal_description = Column(String)
    plan = Column(String)

    # Results
    final_response = Column(String)
    success = Column(Boolean)
    confidence = Column(Float)

    # Metadata
    iterations = Column(Integer)
    tools_used = Column(JSON)  # List of tool names
    state_history = Column(JSON)  # Full state progression

    created_at = Column(TIMESTAMP(timezone=True), server_default=text("now()"))
    completed_at = Column(TIMESTAMP(timezone=True))

# Save after each state transition:
async def _initialize_node(self, state: GraphState) -> GraphState:
    # ... node logic ...

    # Save to database
    await self._save_state(state)

    return state

async def _save_state(self, state: GraphState):
    """Persist current state to database"""
    execution_id = state.get("execution_id")

    if not execution_id:
        # Create new execution record
        execution = AgentExecution(
            user_id=state["user_id"],
            user_request=state["user_request"],
            current_state=state["current_state"].value,
            state_history=[state]
        )
        await db.add(execution)
        await db.commit()
        state["execution_id"] = execution.id
    else:
        # Update existing record
        execution = await db.get(AgentExecution, execution_id)
        execution.current_state = state["current_state"].value
        execution.goal_description = state.get("goal", {}).get("description")
        execution.plan = state.get("plan")
        execution.iterations = state.get("iterations")
        execution.state_history.append(state)  # Append to history
        await db.commit()
```

**Impact:**
- Conversation history for users
- Resume capability (if agent crashes)
- Analytics (see state progressions, bottlenecks)
- Debugging (replay executions)

**Effort:** 6 hours

---

## Implementation Priority

### Week 1 (Sprint Start)
1. âœ… Parallel Tool Execution (1 day) - **HIGHEST IMPACT**
2. âœ… Structured Logging (2 hours)
3. âœ… Better Error Messages (2 hours)
4. âœ… Request Timeout (3 hours)

**Total:** 2.5 days

### Week 2 (Sprint End)
5. âœ… Tool Result Caching (4 hours)
6. âœ… Cost Tracking (4 hours)
7. âœ… State Persistence (6 hours)

**Total:** 2 days

**Sprint Total:** 4.5 days of work

---

## Expected Improvements

| Metric | Before | After Quick Wins | Improvement |
|--------|--------|------------------|-------------|
| Response time (3 tools) | 3.0s | 1.0s | 67% faster |
| Error rate | 8% | 2% | 75% reduction |
| Cost visibility | 0% | 100% | âœ… Full tracking |
| Debugging time | 30 min | 5 min | 83% faster |
| Cache hit rate | 0% | 40% | âœ… New capability |

---

## Next Steps

1. **Review** this document with team
2. **Prioritize** based on urgency (suggest Week 1 order above)
3. **Create tickets** in sprint planning
4. **Implement** in order of priority
5. **Measure** improvements with metrics

**Total ROI:** ~5 days of work for **3x better performance + full observability**

Very high return on investment! ðŸŽ¯
